{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to build and train a Fully convolution Neural Netwok(FCN) to identify a specific person among people. then the quadrotor continues to follow it.\n",
    "\n",
    "FCNs works with three main advantages:\n",
    "- It replaces fully connected layers with 1x1 convolution layers to store spational information\n",
    "- Upsampling through the use of transposed convolutional layers\n",
    "- Skip connections to perserve lost information during convolution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Network Architecture\n",
    "\n",
    "The network architecture is composed of following layers\n",
    "\n",
    "- 2 encoder layers\n",
    "- 1x1 conv layer\n",
    "-  2 decoder layers\n",
    "  \n",
    "Below shows the architechture:\n",
    "![network-architecture](./images/fcn_a.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Model\n",
    "\n",
    "##### The Encoder Block\n",
    "\n",
    "The encoder block includes a separable convolution layer using the separable_conv2d_batchnorm() function:\n",
    "\n",
    "```python\n",
    "def encoder_block(input_layer, filters, strides):\n",
    "    \n",
    "    # TODO Create a separable convolution layer using the separable_conv2d_batchnorm() function.\n",
    "    output_layer = separable_conv2d_batchnorm(input_layer, filters, strides=strides)\n",
    "    return output_layer\n",
    "```\n",
    "\n",
    "as is shown above, the input image size is (160, 160, 3),the Encoder will essentially require separable convolution layers, Separable convolutions, also known as depthwise separable convolutions, comprise of a convolution performed over each channel of an input layer and followed by a 1x1 convolution that takes the output channels from the previous step and then combines them into an output layer.\n",
    "\n",
    "    def separable_conv2d_batchnorm(input_layer, filters, strides=1):\n",
    "            output_layer = SeparableConv2DKeras(filters=filters,kernel_size=3, strides=strides,\n",
    "                                     padding='same', activation='relu')(input_layer)\n",
    "            output_layer = layers.BatchNormalization()(output_layer) \n",
    "            return output_layer\n",
    "            \n",
    "The reduction in the parameters make separable convolutions quite efficient with improved runtime performance and are also, as a result, useful for mobile applications. They also have the added benefit of reducing overfitting to an extent, because of the fewer parameters.\n",
    "\n",
    "Also the separable_conv2d_batchnorm() function adds a batch normalization layer after the separable convolution layer, this is beacuse the batchnorm() has some advantages list below:\n",
    "\n",
    "- Networks train faster – Each training iteration will actually be slower because of the extra calculations during the forward pass. However, it should converge much more quickly, so training should be faster overall.\n",
    "- Allows higher learning rates – Gradient descent usually requires small learning rates for the network to converge. And as networks get deeper, their gradients get smaller during back propagation so they require even more iterations. Using batch normalization allows us to use much higher learning rates, which further increases the speed at which networks train.\n",
    "- Simplifies the creation of deeper networks – Because of the above reasons, it is easier to build and faster to train deeper neural networks when using batch normalization.\n",
    "- Provides a bit of regularization – Batch normalization adds a little noise to your network. In some cases, such as in Inception modules, batch normalization has been shown to work as well as dropout.\n",
    "\n",
    "there are two encode layers,each encoding layer performs a depthwise separable convolution. This requires less compute resources as opposed to using normal convolutions. It is able to accomplish this by significantly reducing the total number of parameters necessary for the computations.\n",
    "\n",
    "##### Decoder Block\n",
    "\n",
    "``` python\n",
    "def decoder_block(small_ip_layer, large_ip_layer, filters):\n",
    "   \n",
    "    # Upsample the small input layer using the bilinear_upsample() function.\n",
    "    upsampled_small_ip_layer = bilinear_upsample(small_ip_layer)\n",
    "    \n",
    "    # Concatenate the upsampled and large input layers using layers.concatenate\n",
    "    concatenated = layers.concatenate([upsampled_small_ip_layer, large_ip_layer])\n",
    "    # Add some number of separable convolution layers\n",
    "    conv_layer_1 = separable_conv2d_batchnorm(concatenated, filters)\n",
    "    conv_layer_2 = separable_conv2d_batchnorm(conv_layer_1, filters)\n",
    "    output_layer = separable_conv2d_batchnorm(conv_layer_2, filters)\n",
    "    \n",
    "    return output_layer\n",
    "```\n",
    "The decoder block is comprised of three parts:\n",
    "- A bilinear upsampling layer using the upsample_bilinear() function. The current recommended factor for upsampling is set to 2.\n",
    "- A layer concatenation step. This step is similar to skip connections. You will concatenate the upsampled small_ip_layer and the large_ip_layer.\n",
    "- Some (one or two) additional separable convolution layers to extract some more spatial information from prior layers.\n",
    "\n",
    "Bilinear upsampling is a resampling technique that utilizes the weighted average of four nearest known pixels, located diagonally to a given pixel, to estimate a new pixel intensity value. The weighted average is usually distance dependent.The bilinear upsampling method does not contribute as a learnable layer like the transposed convolutions in the architecture and is prone to lose some finer details, but it helps speed up performance.\n",
    "One added advantage of concatenating the layers is that it offers a bit of flexibility because the depth of the input layers need not match up unlike when you have to add them. Which helps simplify the implementation as well.\n",
    "\n",
    "Concatenating two layers, the upsampled layer and a layer with more spatial information than the upsampled one,While layer concatenation in itself is helpful for your model, it can often be better to add some regular or separable convolution layers after this step for your model to be able to learn those finer spatial details from the previous layers better.\n",
    "\n",
    "#####  1X1 convolution\n",
    "- In 1x1 convolution the tensor remains 4D instead of getting flattened to 2D. Hence the spatial information is preserved.\n",
    "- A 1x1 convolution is essentially convolving with a set of filters of dimensions:\n",
    "- 1x1xfilter_size (HxWxD), stride = 1, same padding.\n",
    "\n",
    "##### Transposed convolution\n",
    "It is reverse convolution where foward and backward passed are swapped.the decoding layers after this 1x1 layer are later on used to ensure that spatial information is preserved, and that the final output shows where each object is located in the image.\n",
    "\n",
    "#####  Skip Connections\n",
    "The global information is lost in encoder blocks as we look at patches. Skip connection helps to retain that information. The output of one layer is connected to non-adjacent layer.\n",
    "\n",
    "##### Full Model\\\n",
    "\n",
    "After finish previous steps, we add them all together to train the model:\n",
    "``` python\n",
    "def fcn_model(inputs, num_classes):\n",
    "    \n",
    "    # TODO Add Encoder Blocks. \n",
    "    encoder_block1 = encoder_block(inputs, 16, strides=2)\n",
    "    encoder_block2 = encoder_block(encoder_block1,64, strides=2)\n",
    "    # TODO Add 1x1 Convolution layer using conv2d_batchnorm().\n",
    "    one_by_one_conv = conv2d_batchnorm(encoder_block2, 256, kernel_size=1, strides=1)\n",
    "\n",
    "    # TODO: Add the same number of Decoder Blocks as the number of Encoder Blocks\n",
    "    decoder_block1 = decoder_block(one_by_one_conv, encoder_block1, 64)\n",
    "    x = decoder_block(decoder_block1, inputs, 16)\n",
    "    \n",
    "    return layers.Conv2D(num_classes, 1, activation='softmax', padding='same')(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 40\n",
    "num_epochs = 20\n",
    "steps_per_epoch = 200\n",
    "validation_steps = 50\n",
    "workers = 2\n",
    "\n",
    "- learning_rate\n",
    "\n",
    "The learning rate is a value shows how fast a neural network makes adjustments to what it has learned while it is being trained.The ideal learning rate would allow a neural network to reach the gradient descent and error minimization minimums at the least amount of time, without causing overfitting.\n",
    "The learning rate 0.01 is used by the defaut of the project, I also tried 0.005 and 0.001, due to the trainning process is too slow,I trained on my ubuntu 16.04 with gpu 1070, the 0.01 fits for the project requiments.\n",
    "\n",
    "- batch_size\n",
    "\n",
    "The batch size is the number of images that are processed together as a batch for every step of the epoch or the training period. So in this instance, for every step of training, 40 images are taken from the training set and are used for the gradient descent step to minimize the error or loss value.A good batch size would be one that is large enough such that it can still be handled by available computing resources, but at the same time not too large to cause overfitting. It should also be small enough but not too small. If the batches are too small, the neural network may have difficulty forming its generalizations.\n",
    "\n",
    "- num_epochs\n",
    "\n",
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data.Each epoch attempts to move to a lower cost, leading to better accuracy. this is flexiable value to be set up with, due to my previouse experience with deep learning project. 20 is relative reasonable number to start with, so I used 20 with good result to pass the test.\n",
    "\n",
    "- steps_per_epoch\n",
    "\n",
    "steps_per_epoch: Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of unique samples of your dataset divided by the batch size.\n",
    "\n",
    "\n",
    "- validation_steps\n",
    "\n",
    "similar to steps_per_epoch but on the validation data set instead on the training data.\n",
    "This is also the default value provided by the project.\n",
    "\n",
    "- workers\n",
    "\n",
    "max number of used processes\n",
    "This is also the default value provided by the project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Discussion\n",
    "Now lets look at your predictions, and compare them to the ground truth labels and original images. \n",
    "patrol_with_targ: Test how well the network can detect the hero from a distance.\n",
    "\n",
    "patrol_non_targ: Test how often the network makes a mistake and identifies the wrong person as the target.\n",
    "\n",
    "following_images: Test how well the network can identify the target while following them.\n",
    "\n",
    "- images while following the target\n",
    "![1](./images/r1.png)\n",
    "\n",
    "- images while at patrol without target\n",
    "![2](./images/r2.png)\n",
    "\n",
    "- images while at patrol with target\n",
    "![3](./images/r3.png)\n",
    "\n",
    "- Scores for while the quad is following behind the target. \n",
    "\n",
    "        number of validation samples intersection over the union evaulated on 542\n",
    "        average intersection over union for background is 0.9899375885213009\n",
    "        average intersection over union for other people is 0.25547930171195143\n",
    "        average intersection over union for the hero is 0.8132220120343139\n",
    "        number true positives: 528, number false positives: 0, number false negatives: 11\n",
    "\n",
    "- Scores for images while the quad is on patrol and the target is not visable\n",
    "\n",
    "        number of validation samples intersection over the union evaulated on 270\n",
    "        average intersection over union for background is 0.96647913901211\n",
    "        average intersection over union for other people is 0.6293791756770767\n",
    "        average intersection over union for the hero is 0.0\n",
    "        number true positives: 0, number false positives: 31, number false negatives: 0\n",
    "\n",
    "- This score measures how well the neural network can detect the target from far away\n",
    "\n",
    "        number of validation samples intersection over the union evaulated on 322\n",
    "        average intersection over union for background is 0.9824235094965825\n",
    "        average intersection over union for other people is 0.2831436550714913\n",
    "        average intersection over union for the hero is 0.2679280334937\n",
    "\n",
    "\n",
    "    number true positives: 149, number false positives: 0, number false negatives: 152\n",
    "\n",
    "    Sum all the true positives, etc from the three datasets to get a weight for the score = 0.7772675086107922\n",
    "\n",
    "    The IoU for the dataset that never includes the hero is excluded from grading\n",
    "    = final_IoU = (iou1 + iou3)/2 = 0.5405750227640069\n",
    "\n",
    "    The final grade score is \n",
    "    = final_score = final_IoU * weight = 0.42017140116100193\n",
    "    \n",
    "    \n",
    "Training and validation data are all used based in the project, and I didn't additonal images or any other image pre-processing for the project, even though the test is passed, but there are still more things can be imporved, \n",
    "the data sets is only based on provided, so probrobly it can't be genralized very well when switch people, this networks can be used to train different data sets to improve the result of roboustness. Aso the parameters in this project can also be improved with more training variation due to the time limit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoboND]",
   "language": "python",
   "name": "conda-env-RoboND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
